import os
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains.question_answering import load_qa_chain

from langchain.vectorstores import FAISS
from langchain.document_loaders import TextLoader
from langchain.prompts.prompt import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

os.environ['OPENAI_API_KEY'] = 'YOUR-API-KEY'


def ChatGPT_response(query: str) -> str:
    '''
    This code generates a ChatGPT response to a query.
    
    Parameter
    ---------
    query (str): A string query or question related to South African law.

    Returns
    -------
    str: A string response generated by the GPT-3 language model executing a chain of Q&A on the input documents and query.
    '''
    
    template = '''
Instructions: You are a South African Legal Expert. Under no circumstances do you give legal advice. \
Your goal is to give information, discussions and analysis supported by context. \
Your response should be detailed, providing sufficient information for the reader to gain a comprehensive understanding of the Constitution and question. \
If you do not know the answer or the question is out of context, \
say "I do not have the information or knowledge to provide a definitive answer to your question at this time." only and stop there.\

Context: {context}
Question: {question}
Answer: Let's think step by step.
'''
    prompt = PromptTemplate(template=template, input_variables=['question', 'context'])

    try:
        # initialise the chatbot model and load the question-answering chain
        llm = ChatOpenAI(temperature=0.8, model_name='gpt-3.5-turbo')
        chain = load_qa_chain(llm, prompt=prompt, chain_type='stuff')
        
        # initialize the embeddings model for document matching
        embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')
        knowledge_base = FAISS.load_local('knowledge_base', embeddings)
        docs = knowledge_base.similarity_search(query)
        
        # run the question-answering chain on the input documents and query to generate a response
        response = chain.run(input_documents=docs, question=query)
        return response
    
    except Exception as response: 
        return str(response)
    
    
# TODO: Bard_response
    
    
